{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import itertools\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models import Sequential \n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import Dropout, Flatten, Dense \n",
    "from keras import applications \n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import math \n",
    "import datetime\n",
    "import time\n",
    "import queue\n",
    "from multiprocessing import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default dimensions we found online\n",
    "img_width, img_height = 224, 224 \n",
    " \n",
    "#Create a bottleneck file\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# loading up our datasets\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "test_data_dir = 'data/test'\n",
    " \n",
    "# number of epochs to train top model \n",
    "epochs = 7 #this has been changed after multiple model run \n",
    "# batch size used by flow_from_directory and predict_generator \n",
    "batch_size = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0912 15:12:11.385747 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0912 15:12:11.397787 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0912 15:12:11.401747 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0912 15:12:11.425750 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0912 15:12:11.693299 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0912 15:12:11.693299 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading vgc16 model\n",
    "vgg16 = applications.VGG16(include_top=False, weights='imagenet')\n",
    "datagen = ImageDataGenerator(rescale=1. / 255) \n",
    "#needed to create the bottleneck .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Train\\n\\n#__this can take an hour and half to run so only run it once. \\n#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\\nstart = datetime.datetime.now()\\n \\ngenerator = datagen.flow_from_directory( \\n    train_data_dir, \\n    target_size=(img_width, img_height), \\n    batch_size=batch_size, \\n    class_mode=None, \\n    shuffle=False) \\n \\nnb_train_samples = len(generator.filenames) \\nnum_classes = len(generator.class_indices) \\n \\npredict_size_train = int(math.ceil(nb_train_samples / batch_size)) \\n \\nbottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \\n \\nnp.save('bottleneck_features_train.npy', bottleneck_features_train)\\nend= datetime.datetime.now()\\nelapsed= end-start\\nprint ('Time: ', elapsed)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Train\n",
    "\n",
    "#__this can take an hour and half to run so only run it once. \n",
    "#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\n",
    "start = datetime.datetime.now()\n",
    " \n",
    "generator = datagen.flow_from_directory( \n",
    "    train_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None, \n",
    "    shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator.filenames) \n",
    "num_classes = len(generator.class_indices) \n",
    " \n",
    "predict_size_train = int(math.ceil(nb_train_samples / batch_size)) \n",
    " \n",
    "bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train) \n",
    " \n",
    "np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4352 images belonging to 51 classes.\n",
      "Time:  0:18:26.282737\n"
     ]
    }
   ],
   "source": [
    "#Validation\n",
    "\n",
    "#__this can take half an hour to run so only run it once. once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\\n\",\n",
    "    \n",
    "start = datetime.datetime.now()\n",
    "generator = datagen.flow_from_directory(  \n",
    "         validation_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "       \n",
    "nb_validation_samples = len(generator.filenames)  \n",
    "       \n",
    "predict_size_validation = int(math.ceil(nb_validation_samples / batch_size)) \n",
    "       \n",
    "bottleneck_features_validation = vgg16.predict_generator(  \n",
    "         generator, predict_size_validation)  \n",
    "       \n",
    "np.save('bottleneck_features_validation.npy', bottleneck_features_validation)\n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4945 images belonging to 51 classes.\n",
      "Time:  0:20:26.303864\n"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "#__this can take half an hour to run so only run it once. once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__\\n\",\n",
    "    \n",
    "start = datetime.datetime.now()\n",
    "generator = datagen.flow_from_directory(  \n",
    "         test_data_dir,  \n",
    "         target_size=(img_width, img_height),  \n",
    "         batch_size=batch_size,  \n",
    "         class_mode=None,  \n",
    "         shuffle=False)  \n",
    "       \n",
    "nb_test_samples = len(generator.filenames)  \n",
    "       \n",
    "predict_size_test = int(math.ceil(nb_test_samples / batch_size))  \n",
    "       \n",
    "bottleneck_features_test = vgg16.predict_generator(  \n",
    "         generator, predict_size_test)  \n",
    "       \n",
    "np.save('bottleneck_features_test.npy', bottleneck_features_test) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20123 images belonging to 51 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#training data\n",
    "generator_top = datagen.flow_from_directory( \n",
    "   train_data_dir, \n",
    "   target_size=(img_width, img_height), \n",
    "   batch_size=batch_size, \n",
    "   class_mode='categorical', \n",
    "   shuffle=False) \n",
    " \n",
    "nb_train_samples = len(generator_top.filenames) \n",
    "num_classes = len(generator_top.class_indices) \n",
    " \n",
    "# load the bottleneck features saved earlier \n",
    "train_data = np.load('bottleneck_features_train.npy') \n",
    " \n",
    "# get the class labels for the training data, in the original order \n",
    "train_labels = generator_top.classes \n",
    " \n",
    "# convert the training labels to categorical vectors \n",
    "train_labels = to_categorical(train_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4352 images belonging to 51 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#validation data\\n\",\n",
    "generator_top = datagen.flow_from_directory(\n",
    "             validation_data_dir,  \n",
    "             target_size=(img_width, img_height),  \n",
    "             batch_size=batch_size,  \n",
    "             class_mode=None,  \n",
    "             shuffle=False)  \n",
    "       \n",
    "nb_validation_samples = len(generator_top.filenames)  \n",
    "       \n",
    "validation_data = np.load('bottleneck_features_validation.npy')  \n",
    "       \n",
    "    \n",
    "validation_labels = generator_top.classes  \n",
    "validation_labels = to_categorical(validation_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4945 images belonging to 51 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#testing data\\n\",\n",
    "generator_top = datagen.flow_from_directory(  \n",
    "             test_data_dir,  \n",
    "             target_size=(img_width, img_height),  \n",
    "             batch_size=batch_size,  \n",
    "             class_mode=None,  \n",
    "             shuffle=False)  \n",
    "       \n",
    "nb_test_samples = len(generator_top.filenames)  \n",
    "       \n",
    "test_data = np.load('bottleneck_features_test.npy')  \n",
    "       \n",
    "    \n",
    "test_labels = generator_top.classes \n",
    "test_labels = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "W0912 15:51:45.927205 12100 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0912 15:51:46.383285 12100 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 20136 input samples and 20123 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-660b791a1851>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m    validation_data=(validation_data, validation_labels))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m (eval_loss, eval_accuracy) = model.evaluate( \n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    802\u001b[0m             ]\n\u001b[0;32m    803\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    235\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 20136 input samples and 20123 target samples."
     ]
    }
   ],
   "source": [
    "#This is the best model we found. For additional models, check out I_notebook.ipynb\n",
    "start = datetime.datetime.now()\n",
    "model = Sequential() \n",
    "model.add(Flatten(input_shape=train_data.shape[1:])) \n",
    "model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3))) \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "   optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "   metrics=['acc'])\n",
    "history = model.fit(train_data, train_labels, \n",
    "   epochs=7,\n",
    "   batch_size=batch_size, \n",
    "   validation_data=(validation_data, validation_labels))\n",
    "model.save_weights(top_model_weights_path)\n",
    "(eval_loss, eval_accuracy) = model.evaluate( \n",
    "    validation_data, validation_labels, batch_size=batch_size,     verbose=1)\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(eval_accuracy * 100)) \n",
    "#print(\"[INFO] Loss: {}\".format(eval_loss)) \n",
    "end= datetime.datetime.now()\n",
    "elapsed= end-start\n",
    "print ('Time: ', elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing our training and validation\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.ylabel('accuracy') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.ylabel('loss') \n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of neural network\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path):\n",
    "   print(\"[INFO] loading and preprocessing image…\") \n",
    "   image = load_img(file_path, target_size=(224, 224)) \n",
    "   image = img_to_array(image) \n",
    "   image = np.expand_dims(image, axis=0)\n",
    "   image /= 255. \n",
    "   return image\n",
    "\n",
    "def test_single_image(path):\n",
    "  animals = ['অ', 'আ', 'ই', 'ঈ', 'উ', 'ঊ', 'ঋ', 'এ', 'ঐ', 'ও', 'ঔ', 'ক', 'খ', 'গ', 'ঘ', 'ঙ', 'চ', 'ছ', 'জ', \n",
    "            'ঝ', 'ঞ', 'ট', 'ঠ', 'ড', 'ঢ', 'ণ', 'ত', 'থ', 'দ', 'ধ', 'ন', 'প', 'ফ', 'ব', 'ভ', 'ম', 'য', 'র', 'ল', \n",
    "            'শ', 'ষ', 'স', 'হ', 'ড়', 'ঢ়', 'য়', 'া', 'ে', 'ু', 'ূ', 'ৃ', '০', '১', '২', '৩', '৪', '৫', '৬', '৭',\n",
    "             '৮','৯']\n",
    "  images = read_image(path)\n",
    "  time.sleep(.5)\n",
    "\n",
    "\n",
    "  bt_prediction = vgg16.predict(images) \n",
    "  preds = model.predict_proba(bt_prediction)\n",
    "  per = 0\n",
    "  for idx, animal, x in zip(range(0, 61), animals , preds[0]):\n",
    "   if (round (x*100, 2) > per):\n",
    "    per = round (x*100, 2)\n",
    "    container = animal\n",
    "    print ('Here = ', container, per)\n",
    "    \n",
    "   print(\"ID: {}, Label: {} {}%\".format(idx, animal, round(x*100,2) ))\n",
    "\n",
    "\n",
    "  print('Final Decision:')\n",
    "  time.sleep(.5)\n",
    "  for x in range(3):\n",
    "   print('.'*(x+1))\n",
    "   time.sleep(.2)\n",
    "  class_predicted = model.predict_classes(bt_prediction)\n",
    "  class_dictionary = generator_top.class_indices \n",
    "  inv_map = {v: k for k, v in class_dictionary.items()}\n",
    "\n",
    "  if (container == 'ু'):\n",
    "    container = 'ড়'\n",
    "  elif (container == 'ূ'):\n",
    "    container = 'ঢ়'\n",
    "  elif (container == 'ৃ'):\n",
    "    container = 'য়'\n",
    "  elif (container == 'ড়'):\n",
    "    container = 'া'\n",
    "  elif (container == 'ঢ়'):\n",
    "    container = 'ু'\n",
    "  elif (container == 'য়'):\n",
    "    container = 'ূ'\n",
    "  elif (container == 'া'):\n",
    "    container = 'ৃ' \n",
    "\n",
    "  print(\"Character: {}, Label: {}, Accuracy: {}%\".format(class_predicted[0],  container, per))\n",
    "  #if (per >= 30):\n",
    "  print(\"ID: {}, Label: {}, Accuracy: {}%\".format(class_predicted[0],  container, per))  \n",
    "  take.put (container)\n",
    "  return load_img(path)\n",
    "path = (r'C:\\Users\\th4pa\\Bangla\\test - Copy (4).png')\n",
    "test_single_image(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"\"\n",
    "while not take.empty():\n",
    "    word = word + take.get()\n",
    "    \n",
    "print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
